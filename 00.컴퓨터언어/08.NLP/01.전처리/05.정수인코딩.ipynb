{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정수 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dictionary 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딕셔너리 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A barber is a person.',\n",
       " 'a barber is good person.',\n",
       " 'a barber is huge person.',\n",
       " 'he Knew A Secret!',\n",
       " 'The Secret He Kept is huge secret.',\n",
       " 'Huge secret.',\n",
       " 'His barber kept his word.',\n",
       " 'a barber kept his word.',\n",
       " 'His barber kept his secret.',\n",
       " 'But keeping and keeping such a huge secret to himself was driving the barber crazy.',\n",
       " 'the barber went up a huge mountain.']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "sentences = sent_tokenize(raw_text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'barber', 'is', 'a', 'person', '.'],\n",
       " ['a', 'barber', 'is', 'good', 'person', '.'],\n",
       " ['a', 'barber', 'is', 'huge', 'person', '.'],\n",
       " ['he', 'Knew', 'A', 'Secret', '!'],\n",
       " ['The', 'Secret', 'He', 'Kept', 'is', 'huge', 'secret', '.'],\n",
       " ['Huge', 'secret', '.'],\n",
       " ['His', 'barber', 'kept', 'his', 'word', '.'],\n",
       " ['a', 'barber', 'kept', 'his', 'word', '.'],\n",
       " ['His', 'barber', 'kept', 'his', 'secret', '.'],\n",
       " ['But',\n",
       "  'keeping',\n",
       "  'and',\n",
       "  'keeping',\n",
       "  'such',\n",
       "  'a',\n",
       "  'huge',\n",
       "  'secret',\n",
       "  'to',\n",
       "  'himself',\n",
       "  'was',\n",
       "  'driving',\n",
       "  'the',\n",
       "  'barber',\n",
       "  'crazy',\n",
       "  '.'],\n",
       " ['the', 'barber', 'went', 'up', 'a', 'huge', 'mountain', '.']]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 토큰화 (이중리스트는 모르겠음)\n",
    "preprocessed_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'barber',\n",
       " 'is',\n",
       " 'a',\n",
       " 'person',\n",
       " '.',\n",
       " 'a',\n",
       " 'barber',\n",
       " 'is',\n",
       " 'good',\n",
       " 'person',\n",
       " '.',\n",
       " 'a',\n",
       " 'barber',\n",
       " 'is',\n",
       " 'huge',\n",
       " 'person',\n",
       " '.',\n",
       " 'he',\n",
       " 'knew',\n",
       " 'a',\n",
       " 'secret',\n",
       " '!',\n",
       " 'the',\n",
       " 'secret',\n",
       " 'he',\n",
       " 'kept',\n",
       " 'is',\n",
       " 'huge',\n",
       " 'secret',\n",
       " '.',\n",
       " 'huge',\n",
       " 'secret',\n",
       " '.',\n",
       " 'his',\n",
       " 'barber',\n",
       " 'kept',\n",
       " 'his',\n",
       " 'word',\n",
       " '.',\n",
       " 'a',\n",
       " 'barber',\n",
       " 'kept',\n",
       " 'his',\n",
       " 'word',\n",
       " '.',\n",
       " 'his',\n",
       " 'barber',\n",
       " 'kept',\n",
       " 'his',\n",
       " 'secret',\n",
       " '.',\n",
       " 'but',\n",
       " 'keeping',\n",
       " 'and',\n",
       " 'keeping',\n",
       " 'such',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'secret',\n",
       " 'to',\n",
       " 'himself',\n",
       " 'was',\n",
       " 'driving',\n",
       " 'the',\n",
       " 'barber',\n",
       " 'crazy',\n",
       " '.',\n",
       " 'the',\n",
       " 'barber',\n",
       " 'went',\n",
       " 'up',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'mountain',\n",
       " '.']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이중 리스트 빠져나오기\n",
    "words = []\n",
    "\n",
    "for sentence in preprocessed_sentences:\n",
    "    for word in sentence:\n",
    "        words.append(word.lower())\n",
    "        \n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['barber',\n",
       " 'person',\n",
       " '.',\n",
       " 'barber',\n",
       " 'good',\n",
       " 'person',\n",
       " '.',\n",
       " 'barber',\n",
       " 'huge',\n",
       " 'person',\n",
       " '.',\n",
       " 'knew',\n",
       " 'secret',\n",
       " '!',\n",
       " 'secret',\n",
       " 'kept',\n",
       " 'huge',\n",
       " 'secret',\n",
       " '.',\n",
       " 'huge',\n",
       " 'secret',\n",
       " '.',\n",
       " 'barber',\n",
       " 'kept',\n",
       " 'word',\n",
       " '.',\n",
       " 'barber',\n",
       " 'kept',\n",
       " 'word',\n",
       " '.',\n",
       " 'barber',\n",
       " 'kept',\n",
       " 'secret',\n",
       " '.',\n",
       " 'keeping',\n",
       " 'keeping',\n",
       " 'huge',\n",
       " 'secret',\n",
       " 'driving',\n",
       " 'barber',\n",
       " 'crazy',\n",
       " '.',\n",
       " 'barber',\n",
       " 'went',\n",
       " 'huge',\n",
       " 'mountain',\n",
       " '.']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 처리\n",
    "stop_words_list = stopwords.words('english')\n",
    "len(stop_words_list)\n",
    "\n",
    "filtered_words = [word for word in words if word not in stop_words_list]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mountain': 1,\n",
       " 'barber': 8,\n",
       " 'went': 1,\n",
       " 'kept': 4,\n",
       " 'person': 3,\n",
       " 'secret': 6,\n",
       " 'keeping': 2,\n",
       " 'good': 1,\n",
       " 'crazy': 1,\n",
       " 'driving': 1,\n",
       " 'knew': 1,\n",
       " 'huge': 5,\n",
       " 'word': 2}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 카운트값이랑 단어 딕셔너리로 만들기\n",
    "removed_dup_words = set(filtered_words)\n",
    "filtered_words.count('barber')\n",
    "key = [word for word in removed_dup_words]\n",
    "value = [filtered_words.count(word) for word in removed_dup_words]\n",
    "\n",
    "words_dict = {}\n",
    "for key,value in zip(key,value):\n",
    "    if len(key) >= 2:\n",
    "        words_dict[key] = value\n",
    "\n",
    "words_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위에 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mountain': 1,\n",
       " 'barber': 8,\n",
       " 'went': 1,\n",
       " 'kept': 4,\n",
       " 'person': 3,\n",
       " 'secret': 6,\n",
       " 'keeping': 2,\n",
       " 'good': 1,\n",
       " 'crazy': 1,\n",
       " 'driving': 1,\n",
       " 'knew': 1,\n",
       " 'huge': 5,\n",
       " 'word': 2}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "sentences = sent_tokenize(raw_text)\n",
    "sentences\n",
    "\n",
    "# 단어 토큰화 (이중리스트는 모르겠음)\n",
    "preprocessed_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "preprocessed_sentences\n",
    "\n",
    "# 이중 리스트 빠져나오기\n",
    "words = []\n",
    "\n",
    "for sentence in preprocessed_sentences:\n",
    "    for word in sentence:\n",
    "        words.append(word.lower())\n",
    "        \n",
    "# words\n",
    "\n",
    "# 불용어 처리\n",
    "# stop_words_list = stopwords.words('english')\n",
    "# len(stop_words_list)\n",
    "\n",
    "filtered_words = [word for word in words if word not in stop_words_list]\n",
    "filtered_words\n",
    "\n",
    "# 카운트값이랑 단어 딕셔너리로 만들기\n",
    "# removed_dup_words = set(filtered_words)\n",
    "# filtered_words.count('barber')\n",
    "key = [word for word in removed_dup_words]\n",
    "value = [filtered_words.count(word) for word in set(filtered_words)]\n",
    "\n",
    "words_dict = {}\n",
    "for key,value in zip(key,value):\n",
    "    if len(key) >= 2:\n",
    "        words_dict[key] = value\n",
    "\n",
    "words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mountain': 1,\n",
       " 'barber': 8,\n",
       " 'went': 1,\n",
       " 'kept': 4,\n",
       " 'person': 3,\n",
       " 'secret': 6,\n",
       " 'keeping': 2,\n",
       " 'good': 1,\n",
       " 'crazy': 1,\n",
       " 'driving': 1,\n",
       " 'knew': 1,\n",
       " 'huge': 5,\n",
       " 'word': 2}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 코드 더 간결하고 읽기 좋게 만드는 연습\n",
    "sentences = sent_tokenize(raw_text)\n",
    "sentences\n",
    "\n",
    "preprocessed_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "words = []\n",
    "for sentence in preprocessed_sentences:\n",
    "    for word in sentence:\n",
    "        words.append(word.lower())\n",
    "filtered_words = [word for word in words if word not in stop_words_list]\n",
    "\n",
    "key = [word for word in removed_dup_words]\n",
    "value = [filtered_words.count(word) for word in set(filtered_words)]\n",
    "\n",
    "words_dict = {}\n",
    "for key,value in zip(key,value):\n",
    "    if len(key) >= 2:\n",
    "        words_dict[key] = value\n",
    "\n",
    "words_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 교수님"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "sentences = sent_tokenize(raw_text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    results = []\n",
    "    \n",
    "    for word in tokenized_sentence:\n",
    "        word = word.lower()        \n",
    "        if word not in stop_words:\n",
    "            if len(word) > 2:\n",
    "                results.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0\n",
    "                vocab[word] += 1\n",
    "       \n",
    "    preprocessed_sentences.append(results)\n",
    "preprocessed_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 8,\n",
       " 'person': 3,\n",
       " 'good': 1,\n",
       " 'huge': 5,\n",
       " 'knew': 1,\n",
       " 'secret': 6,\n",
       " 'kept': 4,\n",
       " 'word': 2,\n",
       " 'keeping': 2,\n",
       " 'driving': 1,\n",
       " 'crazy': 1,\n",
       " 'went': 1,\n",
       " 'mountain': 1}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 만들어진 딕셔너리 정리 후 빈도수로 순위매겨 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 8,\n",
       " 'secret': 6,\n",
       " 'huge': 5,\n",
       " 'kept': 4,\n",
       " 'person': 3,\n",
       " 'word': 2,\n",
       " 'keeping': 2,\n",
       " 'good': 1,\n",
       " 'knew': 1,\n",
       " 'driving': 1,\n",
       " 'crazy': 1,\n",
       " 'went': 1,\n",
       " 'mountain': 1}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value 값을 기준으로 내림차순 정렬\n",
    "sorted_vocab = dict(sorted(vocab.items(), key=(lambda x:x[1]),reverse=True))\n",
    "sorted_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1,\n",
       " 'secret': 2,\n",
       " 'huge': 3,\n",
       " 'kept': 4,\n",
       " 'person': 5,\n",
       " 'word': 6,\n",
       " 'keeping': 7}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "rank = 0\n",
    "for key,value in sorted_vocab.items():\n",
    "    rank += 1\n",
    "    if value > 1:\n",
    "        word_to_index[key] = rank\n",
    "        \n",
    "    \n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_freq = {}\n",
    "for idx,(key,value) in enumerate(word_to_index.items()):\n",
    "    words_freq[key] = value\n",
    "    if idx+1 > 4:\n",
    "        break\n",
    "words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "words_frequency = dict((key,value) for (key,value) in word_to_index.items() if value >= vocab_size +1)\n",
    "words_frequency\n",
    "\n",
    "for w in words_frequency:\n",
    "    del word_to_index[w]\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOV (Out-of-Vocabulary) : 단어 집합에 없는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['OOV'] = len(word_to_index) + 1\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 만들어진 단어집을 이용해서 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 6, 5],\n",
       " [1, 3, 5],\n",
       " [6, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [6, 6, 3, 2, 6, 1, 6],\n",
       " [1, 6, 3, 6]]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = [[ word_to_index[word] if word in word_to_index else word_to_index['OOV'] for word in sentence] for sentence in preprocessed_sentences]\n",
    "encoded\n",
    "\n",
    "# for sentence in preprocessed_sentences:\n",
    "#     for idx,word in enumerate(sentence):\n",
    "#         if word not in word_to_index:\n",
    "#             sentence[idx] = word_to_index['OOV']\n",
    "#         else:\n",
    "#             sentence[idx] = word_to_index[word]\n",
    "        \n",
    "# preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 6, 5],\n",
       " [1, 3, 5],\n",
       " [6, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [6, 6, 3, 2, 6, 1, 6],\n",
       " [1, 6, 3, 6]]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 교수님 버전\n",
    "\n",
    "encoded_sentences = []\n",
    "preprocessed_sentences2 = preprocessed_sentences\n",
    "\n",
    "for sentence in preprocessed_sentences2:\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        try: \n",
    "            encoded_sentence.append(word_to_index[word])\n",
    "        except KeyError:\n",
    "            encoded_sentence.append(word_to_index['OOV'])\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "encoded_sentences    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 리스트 컴프리헨션 써보기 with chatGPT\n",
    "# vocab = {}\n",
    "# preprocessed_sentences = []\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# sentences = sent_tokenize(raw_text)\n",
    "\n",
    "# for sentence in sentences:\n",
    "#     tokenized_sentence = word_tokenize(sentence)\n",
    "#     results = []\n",
    "    \n",
    "#     for word in tokenized_sentence:\n",
    "#         word = word.lower()        \n",
    "#         if word not in stop_words:\n",
    "#             if len(word) > 2:\n",
    "#                 results.append(word)\n",
    "#                 if word not in vocab:\n",
    "#                     vocab[word] = 0\n",
    "#                 vocab[word] += 1\n",
    "       \n",
    "#     preprocessed_sentences.append(results)\n",
    "# preprocessed_sentences\n",
    "\n",
    "\n",
    "# # Using list comprehension\n",
    "# preprocessed_sentences = [\n",
    "#     [word_to_index[word] if word in word_to_index else word_to_index['OOV'] for word in sentence]\n",
    "#     for sentence in preprocessed_sentences\n",
    "# ]\n",
    "\n",
    "# print(preprocessed_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counter 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# counter 사용을 위해 이중리스트를 일차원 리스트로 변경\n",
    "\n",
    "# words = np.hstack(preprocessed_sentences)\n",
    "all_words_list = sum(preprocessed_sentences, [])\n",
    "print(all_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter: 리스트 안에 있는 단어들을 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "vocab2 = Counter(all_words_list)\n",
    "print(vocab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab2 = vocab2.most_common(vocab_size)\n",
    "vocab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스 부여\n",
    "\n",
    "word_to_index2 = {}\n",
    "rank = 0\n",
    "\n",
    "for item in vocab2:\n",
    "    rank += 1\n",
    "    word_to_index2[item[0]] = rank\n",
    "\n",
    "word_to_index2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK의 FreqDist 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "sentences = sent_tokenize(raw_text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    results = []\n",
    "    \n",
    "    for word in tokenized_sentence:\n",
    "        word = word.lower()        \n",
    "        if word not in stop_words:\n",
    "            if len(word) > 2:\n",
    "                results.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0\n",
    "                vocab[word] += 1\n",
    "       \n",
    "    preprocessed_sentences.append(results)\n",
    "preprocessed_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈도수를 기준으로 sorting한 딕셔너리 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, ...})"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "vocab = FreqDist(np.hstack(preprocessed_sentences))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.probability.FreqDist"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특정 빈도수를 기준으로 필터링하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* .fit_on_texts : 단어 빈도가 높은 순으로 정렬 후 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "# 단어 빈도가 높은 순으로 랭킹\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* word_counts : 단어 빈도 수 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "# 단어 빈도 수 세기\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* texts_to_sequences: 단어 인코딩까지 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "# 단어 숫자로 인코딩(위에서 딕셔너리를 통해 인코딩한 로직이 다 들어있음)\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 빈도 수를 기준으로 필터링 하고 싶을 때 -> 수동 처리 해주어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "# 상위 다섯 번째까지의 단어만 사용 -> num_words를 통해 설정 *반영 시점은 실제 인코딩\n",
    "\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1)\n",
    "\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "# num_words로 상위 다섯번째까지만 쓰겠다는 설정이 인코딩 과정에서 반영됨\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "# word_index와 word_counts에까지 반영하게 하려면\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "\n",
    "# 인덱스가 5 초과인 단어 걸러내기\n",
    "vocab_size = 5\n",
    "words_frequency = [word for word, index in tokenizer.word_index.items() if index >= vocab_size + 1]\n",
    "\n",
    "# 인덱스가 5 초과인 단어 수동으로 제거\n",
    "for word in words_frequency:\n",
    "    del tokenizer.word_index[word]\n",
    "    del tokenizer.word_counts[word]\n",
    "\n",
    "# 결과    \n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.word_counts)\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 결과물 데이터타입 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'collections.OrderedDict'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizer.word_index))\n",
    "print(type(tokenizer.word_counts))\n",
    "print(type(tokenizer.texts_to_sequences(preprocessed_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOV 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 OOV의 인덱스: 1\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "\n",
    "print('단어 OOV의 인덱스: {}'.format(tokenizer.word_index['OOV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OOV': 1,\n",
       " 'barber': 2,\n",
       " 'secret': 3,\n",
       " 'huge': 4,\n",
       " 'kept': 5,\n",
       " 'person': 6,\n",
       " 'word': 7,\n",
       " 'keeping': 8,\n",
       " 'good': 9,\n",
       " 'knew': 10,\n",
       " 'driving': 11,\n",
       " 'crazy': 12,\n",
       " 'went': 13,\n",
       " 'mountain': 14}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
