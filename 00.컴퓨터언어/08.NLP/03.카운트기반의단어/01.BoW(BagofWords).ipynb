{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Words 개요\n",
    "- 통계적 언어 모델 (SLM : Statistical Language Model)\n",
    "    * 확률 분포를 기반으로 언어 이해 (ex 빈도 수 등)\n",
    "    * 모델 예시 : Bow, DTM, TDM, TF-IDF\n",
    "- 신경망 언어 모델 (NNLM : Neural Network Language Model)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (BoW) Bag of Words\n",
    "* 문서 내의 단어들의 순서나 문맥을 고려하지 않음\n",
    "* 단어의 출현 빈도에만 집중\n",
    "* 문서 분류, 정보 검색 등 다양한 작업(순서와 문맥을 고려하지는 않는 작업)에서 사용\n",
    "* 단일 문서에 대한 처리 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTM(Document-Term Matrix)\n",
    "* 텍스트 문서를 단어의 출현 빈도를 기반으로 행렬로 표현\n",
    "* 각 행은 문서, 각 열은 단어\n",
    "* 문서간 유사성 분석, 주제 모델링, 정보 검색 등의 작업에서 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDM(Term-Document Matrix)\n",
    "* 텍스트 문서를 단어의 출현 빈도를 기반으로 행렬로 표현\n",
    "* 각 행은 단어, 각 열은 문서\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (BoW) Bag of Words\n",
    "---\n",
    "* 문서 내의 단어들의 순서나 문맥을 고려하지 않음\n",
    "* 단어의 출현 빈도에만 집중\n",
    "* 문서 분류, 정보 검색 등 다양한 작업(순서와 문맥을 고려하지는 않는 작업)에서 사용\n",
    "* 단일 문서에 대한 처리 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "def build_bag_of_words(document):\n",
    "    # 토큰화 처리\n",
    "    document = document.replace('.','')                 # 구두점 제거\n",
    "    tokenized_document = okt.morphs(document)           # 토큰화\n",
    "    \n",
    "    # BoW와 단어집합 만들기 - 객체 선언\n",
    "    bow = []                                            # bow\n",
    "    word_to_index = {}                                  # 단어 집합\n",
    "    \n",
    "    # Bow와 단어집합 만들기\n",
    "    for word in tokenized_document:                     # 토큰화된 단어 각각 뽑아내기\n",
    "        if word not in word_to_index.keys():            # 단어 집합의 키값에에 토큰화된 단어가 없는 경우\n",
    "            word_to_index[word] = len(word_to_index)        # 단어 집합에 키값 추가, 단어 집합의 현재 길이만큼(=추가된 순서, 0부터 시작)을 밸류값으로 추가\n",
    "            bow.insert(len(word_to_index) - 1, 1)           # bow의 (단어 집합의 현재 길이보다 1 작은 위치 -> 인덱스는 0부터 시작하고 방금 뭔가 추가되어 길이는 1부터 시작하기 때문에)를 인덱스로 하여 1 추가\n",
    "        else:                                           # 단어 집합의 키값에 토큰화된 단어가 이미 있음\n",
    "            index = word_to_index.get(word)                 # 단어 집합의 word를 키값으로하는 밸류를 가져옴(추가된 순서)\n",
    "            bow[index] += 1                                 # bow에 (단어 집합에 추가된 순서)를 인덱스로 해서 1 추가\n",
    "    return word_to_index, bow\n",
    "\n",
    "# list.insert(i, x) 형태로 사용한다. list의 원하는 위치 i 앞에 추가할 값 x를 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
      "bag of words vector:  [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc1 = '정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.'\n",
    "vocab, bow = build_bag_of_words(doc1)\n",
    "\n",
    "print('vocabulary: ', vocab)\n",
    "print('bag of words vector: ', bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  {'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
      "bag of words vector:  [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc2 = '소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.'\n",
    "\n",
    "vocab, bow = build_bag_of_words(doc2)\n",
    "print('vocabulary: ', vocab)\n",
    "print('bag of words vector: ', bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\n",
      "bag of words vector:  [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc3 = doc1 + ' ' + doc2\n",
    "vocab, bow = build_bag_of_words(doc3)\n",
    "\n",
    "print('vocabulary: ', vocab)\n",
    "print('bag of words vector: ', bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### CountVectorize로 BoW 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector:  [[1 1 2 1 2 1]]\n",
      "vocabulary:  {'you': 4, 'knoe': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you knoe I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "print('bag of words vector: ', vector.fit_transform(corpus).toarray())\n",
    "print('vocabulary: ',vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불용어를 제거한 BoW 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 사용자 정의 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector:  [[1 1 1 1 1]]\n",
      "vocabulary:  {'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = [\"Family is not an important thing. It's everything.\"]           # 대상 corpus\n",
    "vect = CountVectorizer(stop_words=['the','a','an','is','not'])          # CountVectorizer 객체 선언하면서 stop_words 속성에 불용어 정의\n",
    "\n",
    "print('bag of words vector: ', vect.fit_transform(text).toarray())\n",
    "print('vocabulary: ',vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CounterVectorizer 내장 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector:  [[1 1 1]]\n",
      "vocabulary:  {'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = [\"Family is not an important thing. It's everything.\"]           # 대상 corpus\n",
    "vect = CountVectorizer(stop_words='english')                            # stopwords 속성에 필터링할 불용어 매칭\n",
    "\n",
    "print('bag of words vector: ', vect.fit_transform(text).toarray())\n",
    "print('vocabulary: ',vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### nltk 내장 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector:  [[1 1 1 1]]\n",
      "vocabulary:  {'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')                                 # nltk stopwords에서 .words 명령어로 불용어 셋 가져오기\n",
    "\n",
    "text = [\"Family is not an important thing. It's everything.\"]           # 대상 corpus\n",
    "vect = CountVectorizer(stop_words=stop_words)                           # stopwords 속성에 필터링할 불용어 매칭\n",
    "\n",
    "print('bag of words vector: ', vect.fit_transform(text).toarray())\n",
    "print('vocabulary: ',vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
